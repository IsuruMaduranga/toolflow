#!/usr/bin/env python3
"""
Simple test for Llama provider without making API calls.
Tests basic functionality and setup.
"""

import sys
sys.path.insert(0, 'src')

try:
    import openai
    import toolflow
    
    print("âœ… Testing Llama provider import...")
    from toolflow import from_llama
    print("âœ… from_llama imported successfully")
    
    print("\nâœ… Testing client validation...")
    
    # Test with invalid client
    try:
        from_llama("invalid")
        print("âŒ Should have failed with invalid client")
    except ValueError as e:
        print(f"âœ… Correctly rejected invalid client: {e}")
    
    # Test with valid client
    client = openai.OpenAI(api_key="test", base_url="https://test.com")
    enhanced_client = from_llama(client)
    print("âœ… Valid client accepted")
    
    print("\nâœ… Testing client structure...")
    print(f"âœ… Client type: {type(enhanced_client)}")
    print(f"âœ… Has chat: {hasattr(enhanced_client, 'chat')}")
    print(f"âœ… Has completions: {hasattr(enhanced_client.chat, 'completions')}")
    print(f"âœ… Has create: {hasattr(enhanced_client.chat.completions, 'create')}")
    
    print("\nâœ… All Llama provider tests passed!")
    print("ğŸ¦™ Llama provider is ready for use!")
    
except Exception as e:
    import traceback
    print(f"âŒ Error: {e}")
    traceback.print_exc()
